# 環境設定
environment:
    grid_world:
        grid_file: C:\Users\enc91\Desktop\hobby\AI\RL\sample\Grid.xlsx # 盤面を定義するファイル。
        # マスの座標は(x座標, y座標)で表し、左上のマスが(0, 0)。
        width: 5 # 盤面の幅。
        height: 5 # 盤面の高さ。
        initial_pos: [0, 0] # 初期座標。
        goal_pos: [[4, 4]] # ゴール座標。複数定義しても良い。
        goal_reward: [100] # ゴール時の報酬。goal_posの要素数と同じにすること。

    cartpole:
        position_part_num: 10 # カート位置の分割数。
        cart_velocity_part_num: 5 # カートの速度の分割数。
        pole_angle_part_num: 10 # 棒の角度の分割数。
        pole_velocity_part_num: 5 # 棒の速度の分割数。
        cart_position_min: -2.4 # カートの位置の最小値(-2.4～2.4の範囲に収めること)。
        cart_position_max: 2.4 # カートの位置の最大値(-2.4～2.4の範囲に収めること)。
        cart_velocity_min: -3.0 # カートの速度の最小値。
        cart_velocity_max: 3.0 # カートの速度の最大値。
        pole_angle_min: -12.0 # 棒の角度(度)の最小値(-12.0～12.0の範囲に収めること)。
        pole_angle_max: 12.0 # 棒の角度(度)の最大値(-12.0～12.0の範囲に収めること)。
        pole_velocity_min: -2.0 # 棒の速度の最小値。
        pole_velocity_max: 2.0 # 棒の速度の最大値。

# エージェントのハイパーパラメータ
agent:
    q_learning:
        num_episode: 10000 # 学習に用いるエピソード数。
        epsilon: 0.1 # ε-greedy法でランダムに行動を選択する確率。
        gamma: 0.8 # 割引率。
        alpha: 0.5 # 学習率。

    sarsa:
        num_episode: 10000 # 学習に用いるエピソード数。
        epsilon: 0.1 # ε-greedy法でランダムに行動を選択する確率。
        gamma: 0.8 # 割引率。
        alpha: 0.5 # 学習率。

    monte_carlo:
        gamma: 0.8 # 割引率。
        epsilon: 0.1 # ε-greedy法でランダムに行動を選択する確率。
        num_playout: 10000 # 学習に用いるプレイアウト数。

    dqn:
        num_episode: 10000 # 学習に用いるエピソード数。
        epsilon: 0.1 # ε-greedy法でランダムに行動を選択する確率。
        gamma: 0.8 # 割引率。
        alpha: 0.01 # 学習率。
        batch_size: 8 # Q Networkの学習時にサンプリングする経験の個数。
        target_update_period: 8 # Target Networkを更新する頻度。
        expbuf_capacity: 1000 # 経験バッファの最大サイズ。
